import requests, time  # , os
from lxml import html  # , etree
download_location = "/home/dom/Code/NHD/download_test/"


class Webpage:
    # Private Functions
    def __init__( self, url=None ):
        if url is None:
            self.tree = None
            self.page = None
        else:
            self._loadWebpage(url)

    def _loadWebpage(self, url):
        headers = {
            'User-Agent': 'My User Agent 1.0',
            'From': 'youremail@domain.com'  # This is another valid field
        }
        cookie = dict(over18="1")
        print("Getting webpage: " + url)
        timedelay = 0
        while( True ):
            if hasattr(self, "page"):
                prevpage = self.page
            time.sleep( timedelay )
            try:
                self.page = requests.get( url, headers=headers, cookies=cookie )
            except ConnectionError:
                time.sleep( 10 )
                self.page = requests.get( url, headers=headers, cookies=cookie )
                if(self.page == prevpage):
                    print('Page failed to load. URL: ' + url)
                    self.page = None
                    self.tree = None
                    return 0
            self.tree = html.fromstring( self.page.content )
            if( self.page.status_code != 429 and self.page.status_code != 404 and self.page.status_code != 503):
                prevpage = self.page
                return 1
            elif( self.page.status_code == 404 ):
                print('Page gives error 404. URL: ' + url)
                return 0
            elif( self.page.status_code == 503):
                print("Website Returned Error 503: Waiting 30 seconds.")
                timedelay *= 20
            else:
                timedelay *= 20
                print("Website Returned Error " + str(self.page.status_code) + ": Waiting " + str(timedelay) + " seconds.")


img = Webpage("http://nht.net/g/229267/").page
with open("1.jpg", "wb+") as f:
    for chunk in img.iter_content(100000):
        f.write(chunk)

# # Download All Pictures
# comic = Webpage("https://nhentai.net/g/229193/")
# total_num_pages = 1 # int( comic.tree.xpath('//div[@id="info"]/div')[0].text_content().split(" ")[0] )
# comic_page = Webpage("https://nhentai.net" + comic.tree.xpath('//a[@class="gallerythumb"]/@href')[0])
# img_link = comic_page.tree.xpath('//section[@id="image-container"]/a/img')[0].attrib["src"]
# img_link_components = img_link.split("/")
# img_link_components_ext = img_link_components[-1].split(".")[-1]
# img_link_components_page_num = int(img_link_components[-1].split(".")[0])
# for i in range(total_num_pages):
    # img = Webpage(img_link).page
    # with open(download_location + str(img_link_components_page_num) + ".jpg", "wb+") as f:
        # for chunk in img.iter_content(100000):
            # f.write(chunk)

    # img_link_components_page_num += 1
    # img_link_components[-1] = str( img_link_components_page_num ) + "." + img_link_components_ext
    # img_link = "/".join( img_link_components )
# # Download Torrent File
# comic = Webpage("https://nhentai.net/g/229267/")
# torerent_link = comic_page.tree.xpath('//section[@id="image-container"]/a/img')[0].attrib["src"]
#
# payload = {'username_or_email': 'pthrowaway44444', 'password': 'DomKippur'}
# with requests.Session() as s:
#     page = s.post( "https://nhentai.net/login/", data=payload )
#     comic = s.get("https://nhentai.net/g/229193/download")
#     comic.content
